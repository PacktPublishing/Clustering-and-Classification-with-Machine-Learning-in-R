ungroup()
library(tidytext)
library(tidyverse)
physics_words = physics %>%
unnest_tokens(word, text) %>%
count(author, word, sort = TRUE) %>%
ungroup()
physics_words
physics_words = physics_words %>%
bind_tf_idf(word, author, n)
plot_physics <- physics_words %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
mutate(author = factor(author, levels = c("Galilei, Galileo",
"Huygens, Christiaan",
"Tesla, Nikola",
"Einstein, Albert")))
physics_words
library(ggplot2)
ggplot(plot_physics[1:20,], aes(tf_idf, word, fill = author, alpha = tf_idf)) +
geom_barh(stat = "identity") +
labs(title = "Highest tf-idf words in Classic Physics Texts",
y = NULL, x = "tf-idf") +
theme_tufte(base_family = "Arial", base_size = 13, ticks = FALSE) +
scale_alpha_continuous(range = c(0.6, 1), guide = FALSE) +
scale_x_continuous(expand=c(0,0)) +
scale_fill_viridis(end = 0.6, discrete=TRUE) +
theme(legend.title=element_blank()) +
theme(legend.justification=c(1,0), legend.position=c(1,0))
install.packages("ggstance")
install.packages("ggthemes")
library(ggstance)
library(ggthemes)
ggplot(plot_physics[1:20,], aes(tf_idf, word, fill = author, alpha = tf_idf)) +
geom_barh(stat = "identity") +
labs(title = "Highest tf-idf words in Classic Physics Texts",
y = NULL, x = "tf-idf") +
theme_tufte(base_family = "Arial", base_size = 13, ticks = FALSE) +
scale_alpha_continuous(range = c(0.6, 1), guide = FALSE) +
scale_x_continuous(expand=c(0,0)) +
scale_fill_viridis(end = 0.6, discrete=TRUE) +
theme(legend.title=element_blank()) +
theme(legend.justification=c(1,0), legend.position=c(1,0))
library(viridis)
ggplot(plot_physics[1:20,], aes(tf_idf, word, fill = author, alpha = tf_idf)) +
geom_barh(stat = "identity") +
labs(title = "Highest tf-idf words in Classic Physics Texts",
y = NULL, x = "tf-idf") +
theme_tufte(base_family = "Arial", base_size = 13, ticks = FALSE) +
scale_alpha_continuous(range = c(0.6, 1), guide = FALSE) +
scale_x_continuous(expand=c(0,0)) +
scale_fill_viridis(end = 0.6, discrete=TRUE) +
theme(legend.title=element_blank()) +
theme(legend.justification=c(1,0), legend.position=c(1,0))
physics = gutenberg_download(c(37729, 14725, 13476, 5001),
meta_fields = "author")
physics_words = physics %>%
unnest_tokens(word, text) %>%
count(author, word, sort = TRUE) %>%
ungroup()
physics_words
physics_words = physics_words %>%
bind_tf_idf(word, author, n)
plot_physics <- physics_words %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
mutate(author = factor(author, levels = c("Galilei, Galileo",
"Huygens, Christiaan",
"Tesla, Nikola",
"Einstein, Albert")))
physics_words
ggplot(plot_physics[1:20,], aes(tf_idf, word, fill = author, alpha = tf_idf)) +
geom_barh(stat = "identity") +
labs(title = "Highest tf-idf words in Classic Physics Texts",
y = NULL, x = "tf-idf") +
theme_tufte(base_family = "Arial", base_size = 13, ticks = FALSE) +
scale_alpha_continuous(range = c(0.6, 1), guide = FALSE) +
scale_x_continuous(expand=c(0,0)) +
scale_fill_viridis(end = 0.6, discrete=TRUE) +
theme(legend.title=element_blank()) +
theme(legend.justification=c(1,0), legend.position=c(1,0))
library(jsonlite)
metadata = fromJSON("https://data.nasa.gov/data.json")
names(metadata$dataset)
library(dplyr)
nasa_title = data_frame(id = metadata$dataset$`_id`$`$oid`,
title = metadata$dataset$title)
nasa_title
nasa_desc =data_frame(id = metadata$dataset$`_id`$`$oid`,
desc = metadata$dataset$description)
nasa_desc %>%
select(desc) %>%
sample_n(10)
library(tidyr)
library(tidytext)
library(tidyverse)
nasa_keyword =data_frame(id = metadata$dataset$`_id`$`$oid`,
keyword = metadata$dataset$keyword) %>% unnest(keyword)
nasa_keyword
nasa_title <- nasa_title %>%
unnest_tokens(word, title) %>%
anti_join(stop_words)
library(widyr)
title_word_pairs = nasa_title %>%
pairwise_count(word, id, sort = TRUE, upper = FALSE)
title_word_pairs
library(ggplot2)
library(igraph)
library(ggraph)
set.seed(1234)
title_word_pairs %>%
filter(n >= 250) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "darkred") +
geom_node_point(size = 5) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
theme_void()
library(SocialMediaLab)
library(magrittr)
library(Rfacebook)
library(plyr)
library(stringr)
appID = "148566689263404" # Put your own api info here
appSecret = "6b9de33ef5fd85bb40d5a8d2ef4ad294" # Put your own api info here
fb_oauth=AuthenticateWithFacebookAPI(appID = fb.appid,
appSecret = fb.appsecret,
extended_permissions = FALSE, # Public Info
useCachedToken = TRUE)        # Use existing
fb_oauth=AuthenticateWithFacebookAPI(appID = appID,
appSecret = appSecret,
extended_permissions = FALSE, # Public Info
useCachedToken = TRUE)        # Use existing
trump.network.df <- CollectDataFacebook(pageName="DonaldTrump",
rangeFrom="2016-03-01",
rangeTo="2016-03-03",
writeToFile=FALSE,
verbose=TRUE,
n=5)
trump.network.df <- CollectDataFacebook(pageName="DonaldTrump",
rangeFrom="2016-03-01",
rangeTo="2017-03-03",
writeToFile=FALSE,
verbose=TRUE,
n=5)
trump.network <- CreateBimodalNetwork(trump.network.df)
library(igraph)
library(tidygraph)
library(ggraph)
trump.network <- CreateBimodalNetwork(trump.network.df)
trump.network.df <- CollectDataFacebook(pageName="DonaldTrump",
rangeFrom="2016-03-01",
rangeTo="2016-03-03",
writeToFile=FALSE,
verbose=TRUE,
n=5)# Use existing
trump.network.df = CollectDataFacebook(pageName="DonaldTrump",
rangeFrom="2016-03-01",
rangeTo="2017-03-03",
writeToFile=FALSE,
verbose=TRUE,
n=5)# Use existing
head(trump.network.df)
trump.network.df
setwd("F:\\SIBERIA\\test1\\NDVI\\Bands\\Field_data\\elevation")
p=read.csv("Species_dNBR_Terrain.csv")
head(p)
names(p)
nameList=c("Density","Percent_Aspen","Percent_Pine","Percent_Larch","Percent_Birch",
"dNBR", "NBR_pre","NDVI_pre",
"EVI_BurnYear","NDVI_BurnYear","NDMI_BurnYear","NBR_BurnYear",
"EVI_2016","NDVI_2016","NDMI_2016","NBR_2016","aspect","slope","elevation")
df1=p[,colnames(p) %in% nameList]
head(df1)
df1$dNBR=as.numeric(df1$dNBR)
df1$elevation=as.numeric(df1$elevation)
r=subset(p,State=="R")
nr=subset(p,State=="NR")
df2=nr[,colnames(nr) %in% nameList]#nr
df2$dNBR=as.numeric(df2$dNBR)
df2$elevation=as.numeric(df2$elevation)
df3=r[,colnames(r) %in% nameList]#r
df3$elevation=as.numeric(df3$elevation)
library(caret)
library(FSelector)
library(FSelector)
library(NeuralNetTools)
library(neuralnet)
normal=function(x){
return((x-min(x))/(max(x)-min(x)))}
con_norm=as.data.frame(lapply(df1,normal))
head(con_norm)
con_norm2=as.data.frame(lapply(df2,normal))#nr
head(con_norm2)
con_norm3=as.data.frame(lapply(df3,normal))#R
weights = rank.correlation(Percent_Aspen~dNBR+ NBR_pre + NDVI_pre+
EVI_BurnYear + NDMI_BurnYear+NBR_BurnYear+aspect+slope+elevation+
NDVI_BurnYear+EVI_2016+NDVI_2016+NDMI_2016+NBR_2016, df1)
print(weights)
subset = cutoff.k(weights, 7)
f = as.simple.formula(subset, "Percent_Aspen")
print(f)
set.seed(1)
concrete_model2 = neuralnet(formula = Percent_Aspen ~ NDMI_BurnYear + NDMI_2016 + NBR_BurnYear + EVI_2016 +
aspect + NDVI_2016 + NBR_pre,
data = con_norm,hidden=5)
olden(concrete_model2)
weights3 = rank.correlation(Percent_Larch~dNBR+ NBR_pre + NDVI_pre+
EVI_BurnYear + NDMI_BurnYear+NBR_BurnYear+aspect+slope+elevation+
NDVI_BurnYear+EVI_2016+NDVI_2016+NDMI_2016+NBR_2016, df3)
print(weights3)
subset3 = cutoff.k(weights3, 7)
f3 = as.simple.formula(subset3, "Percent_Larch")
print(f3)
set.seed(2)
r_regen = neuralnet(formula = Percent_Larch ~ NBR_pre + EVI_2016 + dNBR + NBR_BurnYear + NDMI_BurnYear +
NDVI_BurnYear + NDVI_2016,
data = con_norm3,hidden=5)
olden(r_regen)
set.seed(3)
r_regen = neuralnet(formula = Percent_Larch ~ NBR_pre + EVI_2016 + dNBR + NBR_BurnYear + NDMI_BurnYear +
NDVI_BurnYear + NDVI_2016,
data = con_norm3,hidden=5)
olden(r_regen)
install.packages("darch")
install.packages("h2o")
library(h2o)
install.packages("deepnet")
library(deepnet)
install.packages("DEEPR")
install.packages("keras")
library(keras)
install_keras()
m=dataset_mnist()
install.packages("kohonen")
install.packages("RSNNS")
library(rnn)
partialPlot(x,data3,cf_cvg,"R")
library(caret)
library(randomForest)
library(corrplot)
library(e1071)
x= randomForest(Rec_Coding ~ .,  ntree = 200, data = data3,importance=T)
df1=read.csv("nightL_Sites.csv")
mat_a = subset(df1, select = -c(Rec_Coding))
numeric <- mat_a[sapply(mat_a, is.numeric)]
descrCor <- cor(numeric)
print(descrCor)
highlyCorrelated = findCorrelation(descrCor, cutoff=0.7)
highlyCorCol = colnames(numeric)[highlyCorrelated]
highlyCorCol
data3 = df1[, -which(colnames(df1) %in% highlyCorCol)]
set.seed(10) #reproducibility
inTraining <- createDataPartition(data3$Rec_Coding, p = .75, list = FALSE) #create a 75% data partition
training <- data3[ inTraining,] #75% data for model training
testing  <- data3[-inTraining,] #25% for model testing
x= randomForest(Rec_Coding ~ .,  ntree = 200, data = data3,importance=T)
partialPlot(x,data3,accessibility,"NR")
partialPlot(x,data3,friction,"R")
partialPlot(x,data3,cf_cvg,"R")
partialPlot(x,data3,cf_cvg,"NR")
fs=read.csv("paspen.csv")
head(fs)
nameList=c("Percent_Aspen","accessibility","friction","avg_lights_x_pct","avg_vis",
"cf_cvg", "stable_lights","density_2000","TVFC2016","dNBR",
"density_2015","Ave_ssm_2016","Ave_susm._2016","treecover2000",
"EVI_2016","NDVI_2016","NDMI_2016","NBR_2016","SATVI2016",
"CosineAspect","slope","elevation","NBR_pre","NDVI_pre","EVI_BurnYear",
"NDVI_BurnYear","NDMI_BurnYear")
dfa=fs[,colnames(fs) %in% nameList]
head(dfa)
summary(dfa)
dfa=na.omit(dfa)
mat_a = subset(dfa, select = -c(Percent_Aspen))
numeric <- mat_a[sapply(mat_a, is.numeric)]
descrCor <- cor(numeric)
print(descrCor)
library(corrplot)
corrplot(descrCor)
highlyCorrelated = findCorrelation(descrCor, cutoff=0.7)
highlyCorCol = colnames(numeric)[highlyCorrelated]
highlyCorCol
dfa1 = dfa[, -which(colnames(dfa) %in% highlyCorCol)]
inTraining <- createDataPartition(dfa1$Percent_Aspen, p = .75, list = FALSE) #create a 75% data partition
set.seed(10) #reproducibility
inTraining <- createDataPartition(dfa1$Percent_Aspen, p = .75, list = FALSE) #create a 75% data partition
training <- dfa1[ inTraining,] #75% data for model training
testing  <- dfa1[-inTraining,] #25% for model testing
str(testing)
fitSvm = svm(Percent_Aspen ~ ., data = training)
fitSvm
predictions <- predict(fitSvm, testing[,2:21]
table(predictions, testing$Percent_Aspen)
predictions <- predict(fitSvm, testing[,2:21])
p=as.data.frame(predictions)
final=cbind(testing$Percent_Aspen,p) #how do predicted values (p) compare with actual test data
head(final)
cor(testing$Percent_Aspen,p)
str(training)
scl <- function(x){ (x - min(x))/(max(x) - min(x)) }
tr <- data.frame(lapply(training[, 1:21], scl))
head(tr)
n <- names(tr)
f <- as.formula(paste("Percent_Aspen ~", paste(n[!n %in% c("l1","l2","l3")], collapse = " + ")))
f
library(neuralnet)
nn <- neuralnet(Percent_Aspen ~ accessibility + friction + cf_cvg +
density_2015 + Ave_ssm_2016 + treecover2000 + SATVI2016 +
CosineAspect + dNBR + NBR_pre + NDVI_pre + EVI_BurnYear +
NDVI_BurnYear + NDMI_BurnYear + EVI_2016 + NDVI_2016 + NDMI_2016 +
NBR_2016 + elevation + slope,
data = tr,
hidden =5)
library(NeuralNetTools)
olden(nn)
nn1 <- neuralnet(Percent_Aspen ~ accessibility + friction +  + cf_cvg +density_2015,
data = tr,
hidden =5)
olden(nn1)
head(dfa)
head(df1)
head(dfa)
head(df1)
c=df1[, 'Rec_Coding', drop=FALSE]
head(c)
x=cbind(c,dfa)
head(x)
nr_aspen=subset(x,Rec_Coding=="NR")
nr_a=subset(x,Rec_Coding=="NR")
head(nr_a)
str(nr_a)
scl <- function(x){ (x - min(x))/(max(x) - min(x)) }
tr_a <- data.frame(lapply(nr_a[, 2:28], scl))
head(tr_a)
nn1 <- neuralnet(Percent_Aspen ~ accessibility + friction +  + cf_cvg +density_2015,
data = tr_a,
hidden =5)
olden(nn1)
nn2 <- neuralnet(Percent_Aspen ~ Ave_ssm_2016 + treecover2000 + SATVI2016  + CosineAspect + dNBR,
data = tr_a,
hidden =5)
olden(nn2)
r_a=subset(x,Rec_Coding=="R")
head(r_a)
scl <- function(x){ (x - min(x))/(max(x) - min(x)) }
tr_b <- data.frame(lapply(r_a[, 2:28], scl))
head(tr_b)
nn1 <- neuralnet(Percent_Aspen ~ accessibility + friction +  + cf_cvg +density_2015,
data = tr_b,
hidden =5)
olden(nn1)
nn2 <- neuralnet(Percent_Aspen ~ Ave_ssm_2016 + treecover2000 + SATVI2016  + CosineAspect + dNBR,
data = tr_b,
hidden =5)
olden(nn2)
regen=read.csv("regen_burn.csv")
head(regen)
head(x)
pine=regen[, 'Percent_Pine', drop=FALSE]
pinedata=cbind(x,pine)
head(pine)
pinedata=rbind(x,pine)
head(dfa)
str(dfa)
str(pine)
setwd("F:\\SIBERIA\\Papers\\manuscript2\\data_man2")
pine=read.csv("regen_burn.csv")
head(pine)
pine_nr=subet(pine,State=="NR")
pine_nr=subset(pine,State=="NR")
str(pine)
scl <- function(x){ (x - min(x))/(max(x) - min(x)) }
tr_b <- data.frame(lapply(pine[, 5:18], scl))
pine=read.csv("Fieldparameter_anthro.csv")
head(pine)
str(pine)
d=read.csv("Fieldparameter_anthro.csv")
head(d)
str(d)
d$dNBR=as.numeric(d$dNBR)
d$elevation=as.numeric(d$elevation)
head(d)
scl <- function(x){ (x - min(x))/(max(x) - min(x)) }
tr_b <- data.frame(lapply(d[, 3:41], scl))
head(tr_b)
d=read.csv("Fieldparameter_anthro.csv")
head(d)
str(d)
d$dNBR=as.numeric(d$dNBR)
d$elevation=as.numeric(d$elevation)
summary(d)
d=na.omit(d)
tr_b <- data.frame(lapply(d[, 3:41], scl))
head(tr_b)
nn1 <- neuralnet(Percent_Pine ~ accessibility + friction +  + cf_cvg +density_2015,
data = tr_b,
hidden =5)
library(neuralnet)
nn1 <- neuralnet(Percent_Pine ~ accessibility + friction +  + cf_cvg +density_2015,
data = tr_b,
hidden =5)
library(NeuralNetTools)
olden(nn1)
nn2 <- neuralnet(Percent_Pine ~ Ave_ssm_2016 + treecover2000 +CosineAspect + dNBR,
data = tr_b,
hidden =5)
olden(nn2)
nn1 <- neuralnet(Percent_Larch ~ accessibility + friction +  + cf_cvg +density_2015,
data = tr_b,
hidden =5)
olden(nn1)
nn2 <- neuralnet(Percent_Larch ~ Ave_ssm_2016 + treecover2000 +CosineAspect + dNBR,
data = tr_b,
hidden =5)
olden(nn2)
str(r_a)
head(d)
summary(d)
dnr=subset(d,State="NR")
str(dnr)
scl <- function(x){ (x - min(x))/(max(x) - min(x)) }
tr_a <- data.frame(lapply(dnr[, 3:41], scl))
head(tr_a)
nn1 <- neuralnet(Percent_Pine ~ accessibility + friction +  + cf_cvg +density_2015,
data = tr_a,
hidden =5)
olden(nn1)
nn2 <- neuralnet(Percent_Pine ~ Ave_ssm_2016 + treecover2000 +CosineAspect + dNBR,
data = tr_a,
hidden =5)
olden(nn2)
nn2 <- neuralnet(Percent_Pine ~ Ave_ssm_2016 + treecover2000 +CosineAspect + dNBR,
data = tr_a,
hidden =5)
olden(nn2)
nn1 <- neuralnet(Percent_Larch ~ accessibility + friction +  + cf_cvg +density_2015,
data = tr_a,
hidden =5)
olden(nn1)
nn2 <- neuralnet(Percent_Larch ~ Ave_ssm_2016 + treecover2000 +CosineAspect + dNBR,
data = tr_a,
hidden =5)
olden(nn2)
dr=subset(d,State="R")
tr_a <- data.frame(lapply(dr[, 3:41], scl))
head(tr_a)
nn1 <- neuralnet(Percent_Pine ~ accessibility + friction +  + cf_cvg +density_2015,
data = tr_a,
hidden =5)
olden(nn1)
nn2 <- neuralnet(Percent_Pine ~ Ave_ssm_2016 + treecover2000 +CosineAspect + dNBR,
data = tr_a,
hidden =5)
olden(nn2)
nn1 <- neuralnet(Percent_Larch ~ accessibility + friction +  + cf_cvg +density_2015,
data = tr_a,
hidden =5)
olden(nn1)
nn2 <- neuralnet(Percent_Larch ~ Ave_ssm_2016 + treecover2000 +CosineAspect + dNBR,
data = tr_a,
hidden =5)
olden(nn2)
head(swiss)
swiss = swiss[,-1] #remove categorical variable
swiss.svd = svd(swiss)
plot(swiss.svd$d^2/sum(swiss.svd$d^2), type="l", xlab=" Singular
vector", ylab = "Variance explained")
plot(cumsum(swiss.svd$d^2/sum(swiss.svd$d^2)), type="l",
xlab="Singular vector", ylab = "Cumulative percent of variance
explained")
setwd("F:\\cluster_class-R\\Data\\section6")
install.packages("bmp")
install.packages("RnavGraphImageData")
install.packages("vegan")
library(RnavGraphImageData)
library(vegan)
data(digits)
install.packages("mlbench")
library(mlbench)
library(fpc)
require('ggplot2')
require('lattice')
require('fpc')
dbscan(iris[3:4], 0.15, 3, showplot=1)
dbscan(iris[3:4], 0.2, 4, showplot=1)
dbscan(iris[2:3],0.3, 17, showplot=1)
z<-data.frame(a=iris[,1],b=iris[,2])
r=dbscan(iris[1:4], 0.7, 15)
install.packages("dbscan")
library("dbscan")
data("iris")
x <- as.matrix(iris[, 1:4])
head(x)
db <- dbscan(x, eps = .4, minPts = 4)
db
pairs(x, col = db$cluster + 1L)
lof <- lof(x, k = 4)
pairs(x, cex = lof)
dbscan::kNNdistplot(x, k =  5)
abline(h = 0.15, lty = 2)
pairs(x, cex = lof)
res.fpc <- fpc::dbscan(iris, eps = 0.4, MinPts = 4)
res.fpc <- fpc::dbscan(x, eps = 0.4, MinPts = 4)
res.db <- dbscan::dbscan(iris, 0.4, 4)
res.db <- dbscan::dbscan(x, 0.4, 4)
fviz_cluster(res.fpc, iris, geom = "point")
require(fpc)
fviz_cluster(res.fpc, iris, geom = "point")
require(factoextra)
fviz_cluster(res.fpc, iris, geom = "point")
fviz_cluster(res.fpc, x, geom = "point")
